#!/usr/bin/env python3
"""
Daily Company Crawler with Slack Notification
Automatically runs the crawler and sends formatted results to Slack
"""

import sys
import json
import os
from datetime import datetime
from io import StringIO
import requests
import subprocess
from pathlib import Path

# Add company_crawler to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'company_crawler'))

from physical_intelligence.main import run as run_pi
from skild_ai.main import run as run_skild
from dyna.main import run as run_dyna

COMPANIES = {
    "physical_intelligence": ("Physical Intelligence", run_pi, "pi"),
    "skild_ai": ("Skild AI", run_skild, "skild"),
    "dyna": ("DYNA", run_dyna, "dyna"),
}


def crawl_all_companies(purpose="all"):
    """Run crawler for all companies"""
    results = []

    for key, (name, runner, file_prefix) in COMPANIES.items():
        print(f"\n[INFO] Crawling {name}...")
        try:
            result = runner(purpose)

            # Analyze position changes if there are updates
            position_data = result.get("position", {})
            if position_data and position_data.get("status") == "updated":
                print(f"[INFO] Analyzing position changes for {name}...")
                analysis = analyze_position_changes(name, key, file_prefix, position_data)
                if analysis:
                    result["analysis"] = analysis
                    print(f"[INFO] Analysis completed for {name}")

            results.append(result)
            print(f"[INFO] {name} completed")
        except Exception as e:
            print(f"[ERROR] {name} failed: {e}")
            results.append({"company": name, "error": str(e)})

    return results


def analyze_position_changes(company_name, company_key, file_prefix, position_data):
    """
    Use Claude CLI to analyze position changes and generate insights.

    Args:
        company_name: Full company name (e.g., "Physical Intelligence")
        company_key: Company key for directory paths (e.g., "physical_intelligence")
        file_prefix: File prefix for data files (e.g., "pi")
        position_data: Position data from crawler result

    Returns:
        str: Analysis report in bullet point format, or None if no analysis needed
    """
    status = position_data.get("status", "")

    # Only analyze if there are actual changes
    if status != "updated":
        return None

    # Read full position data
    data_file = Path(f"data/{company_key}/{file_prefix}_positions.json")
    if not data_file.exists():
        return None

    try:
        with open(data_file, 'r', encoding='utf-8') as f:
            full_positions = json.load(f)
    except Exception as e:
        print(f"[ERROR] Failed to read position data for {company_name}: {e}")
        return None

    # Extract changes
    added = position_data.get("added", [])
    removed = position_data.get("removed", [])
    updated = position_data.get("updated", [])

    # Build analysis prompt
    prompt = f"""ë‹¤ìŒì€ {company_name}ì˜ ì±„ìš©ê³µê³  ë³€í™” ë°ì´í„°ì…ë‹ˆë‹¤.

=== ì „ì²´ í˜„ì¬ ì±„ìš©ê³µê³  ===
{json.dumps(full_positions, ensure_ascii=False, indent=2)}

=== ì´ë²ˆì— ê°ì§€ëœ ë³€í™” ===
ì‹ ê·œ ì¶”ê°€: {len(added)}ê°œ
ì‚­ì œë¨: {len(removed)}ê°œ
ì—…ë°ì´íŠ¸ë¨: {len(updated)}ê°œ

ì¶”ê°€ëœ í¬ì§€ì…˜:
{json.dumps(added, ensure_ascii=False, indent=2)}

ì‚­ì œëœ í¬ì§€ì…˜:
{json.dumps(removed, ensure_ascii=False, indent=2)}

ì—…ë°ì´íŠ¸ëœ í¬ì§€ì…˜:
{json.dumps(updated, ensure_ascii=False, indent=2)}

=== ë¶„ì„ ìš”ì²­ ===
ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:

*í˜•ì‹ ê·œì¹™:*
- *Slack mrkdwn í˜•ì‹ ì‚¬ìš©* (ì¤‘ìš”!)
- BoldëŠ” *í…ìŠ¤íŠ¸* (ë³„í‘œ 1ê°œ)
- ** ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
- ìµœëŒ€í•œ ê°„ê²°í•˜ê³  í•µì‹¬ì ì¸ ë¶ˆë › í¬ì¸íŠ¸ë§Œ
- ê° ë¶ˆë ›ì€ í•œ ì¤„ë¡œ ìš”ì•½
- ê° ë¶ˆë › ì•„ë˜ì— ê°„ê²°í•œ í•œì¤„ ê·¼ê±° (ì„œë¸Œ ë¶ˆë ›)
- ì„¤ëª…í˜• ë¬¸ì¥ë³´ë‹¤ëŠ” ê°•ì¡°ëœ í‚¤ì›Œë“œì™€ ìˆ˜ì¹˜ ì¤‘ì‹¬
- ë†€ë„ë§Œí•œ ì¸ì‚¬ì´íŠ¸ë‚˜ íŠ¹ì´ì‚¬í•­ë§Œ í¬í•¨

*ë¶„ì„ í•­ëª©:*
1. ğŸ” ì£¼ìš” ë³€í™”
2. ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸
3. ğŸ“Š ì±„ìš© íŠ¸ë Œë“œ

*ì¶œë ¥ ì˜ˆì‹œ:*
ğŸ” ì£¼ìš” ë³€í™”
â€¢ *Senior ML Engineer* 3ê°œ í¬ì§€ì…˜ ì‹ ê·œ ì¶”ê°€
  - ëª¨ë‘ robotics foundation model ê²½í—˜ ìš”êµ¬
â€¢ Hardware ê´€ë ¨ í¬ì§€ì…˜ 2ê°œ ì‚­ì œ
  - ì†Œí”„íŠ¸ì›¨ì–´ ì¤‘ì‹¬ìœ¼ë¡œ ì „í™˜ ì‹ í˜¸

ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸
â€¢ *Production ë‹¨ê³„ ì§„ì… ê°€ëŠ¥ì„±*
  - DevOps/MLOps ì—”ì§€ë‹ˆì–´ ì±„ìš© ì‹œì‘
â€¢ *ë°ì´í„° ì¸í”„ë¼ ê°•í™” ì¤‘*
  - Data Engineer, ML Platform í¬ì§€ì…˜ ì¶”ê°€

ğŸ“Š ì±„ìš© íŠ¸ë Œë“œ
â€¢ ì „ì²´ ì±„ìš© ê·œëª¨ 40% ì¦ê°€
  - ì´ì „ 10ê°œ â†’ í˜„ì¬ 14ê°œ í¬ì§€ì…˜

ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”. ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ, í•µì‹¬ë§Œ! *ë³„í‘œëŠ” 1ê°œë§Œ* ì‚¬ìš©í•˜ì„¸ìš”."""

    try:
        # Call Claude CLI (using haiku for faster response)
        print(f"[INFO] Sending analysis request to Claude...")
        result = subprocess.run(
            ['claude', '-p', '--model', 'haiku'],
            input=prompt,
            capture_output=True,
            text=True,
            timeout=90  # Increased timeout
        )

        if result.returncode == 0 and result.stdout.strip():
            analysis_text = result.stdout.strip()

            # Save analysis to file for debugging
            analysis_dir = Path("logs/analysis")
            analysis_dir.mkdir(parents=True, exist_ok=True)
            analysis_file = analysis_dir / f"{company_key}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
            analysis_file.write_text(analysis_text, encoding='utf-8')
            print(f"[INFO] Analysis saved to {analysis_file}")

            return analysis_text
        else:
            print(f"[ERROR] Claude CLI failed for {company_name}: {result.stderr}")
            return None

    except subprocess.TimeoutExpired:
        print(f"[ERROR] Claude CLI timeout for {company_name}")
        return None
    except Exception as e:
        print(f"[ERROR] Failed to analyze {company_name}: {e}")
        return None


def format_slack_message(results):
    """Format crawling results for Slack with detailed status for all 7 sections"""

    # Slack Block Kit format
    blocks = [
        {
            "type": "header",
            "text": {
                "type": "plain_text",
                "text": f"ğŸ¤– Daily Company Crawler Report",
                "emoji": True
            }
        },
        {
            "type": "context",
            "elements": [
                {
                    "type": "mrkdwn",
                    "text": f"*Date:* {datetime.now().strftime('%Y-%m-%d %H:%M:%S KST')}"
                }
            ]
        },
        {
            "type": "divider"
        }
    ]

    has_updates = False

    for result in results:
        if not result:
            continue

        company = result.get("company", "Unknown")
        company_text = f"*{company}*\n"

        if "error" in result:
            company_text += f"âŒ *Error:* {result['error']}\n"
            blocks.append({
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": company_text
                }
            })
            blocks.append({"type": "divider"})
            continue

        # Team updates (PI only)
        team_data = result.get("team", {})
        if team_data:
            status = team_data.get("status", "initialized")
            if status == "updated":
                has_updates = True
                added = team_data.get("added", [])
                removed = team_data.get("removed", [])

                company_text += "- *Team:*\n"
                if added:
                    company_text += "  - *Added:*\n"
                    for member in added:
                        name = member.get('name', 'Unknown')
                        company_text += f"    â€¢ {name}\n"
                if removed:
                    company_text += "  - *Removed:*\n"
                    for member in removed:
                        name = member.get('name', 'Unknown')
                        company_text += f"    â€¢ {name}\n"
            else:
                company_text += "- *Team:* Checked\n"

        # Blog/Research updates
        blog_data = result.get("blog") or result.get("research", {})
        if blog_data:
            status = blog_data.get("status", "initialized")
            if status == "updated":
                has_updates = True
                added = blog_data.get("added", [])
                removed = blog_data.get("removed", [])
                updated = blog_data.get("updated", [])

                company_text += "- *Blog:*\n"
                if added:
                    company_text += "  - *Added:*\n"
                    for post in added:
                        title = post.get('title', 'Untitled')
                        url = post.get('url', '')
                        if url:
                            company_text += f"    â€¢ <{url}|{title}>\n"
                        else:
                            company_text += f"    â€¢ {title}\n"
                if removed:
                    company_text += "  - *Removed:*\n"
                    for post in removed:
                        title = post.get('title', 'Untitled')
                        url = post.get('url', '')
                        if url:
                            company_text += f"    â€¢ <{url}|{title}>\n"
                        else:
                            company_text += f"    â€¢ {title}\n"
                if updated:
                    company_text += "  - *Updated:*\n"
                    for post in updated:
                        title = post.get('title', 'Untitled')
                        url = post.get('url', '')
                        if url:
                            company_text += f"    â€¢ <{url}|{title}>\n"
                        else:
                            company_text += f"    â€¢ {title}\n"
            else:
                company_text += "- *Blog:* Checked\n"

        # Position updates
        pos_data = result.get("position", {})
        if pos_data:
            status = pos_data.get("status", "initialized")
            if status == "updated":
                has_updates = True
                added = pos_data.get("added", [])
                removed = pos_data.get("removed", [])
                updated = pos_data.get("updated", [])

                company_text += "- *Career:*\n"
                if added:
                    company_text += "  - *Added:*\n"
                    for pos in added:
                        title = pos.get('title', 'Untitled')
                        url = pos.get('url', '')
                        if url:
                            company_text += f"    â€¢ <{url}|{title}>\n"
                        else:
                            company_text += f"    â€¢ {title}\n"
                if removed:
                    company_text += "  - *Removed:*\n"
                    for pos in removed:
                        title = pos.get('title', 'Untitled')
                        url = pos.get('url', '')
                        if url:
                            company_text += f"    â€¢ <{url}|{title}>\n"
                        else:
                            company_text += f"    â€¢ {title}\n"
                if updated:
                    company_text += "  - *Updated:*\n"
                    for pos in updated:
                        title = pos.get('title', 'Untitled')
                        url = pos.get('url', '')
                        if url:
                            company_text += f"    â€¢ <{url}|{title}>\n"
                        else:
                            company_text += f"    â€¢ {title}\n"
            else:
                company_text += "- *Career:* Checked\n"

        # Add AI analysis if available
        analysis = result.get("analysis")
        if analysis:
            company_text += "\nğŸ“Š *AI Analysis*\n"
            company_text += f"{analysis}\n"

        # Add company section
        blocks.append({
            "type": "section",
            "text": {
                "type": "mrkdwn",
                "text": company_text
            }
        })

        # Add blank line (divider) after each company
        blocks.append({"type": "divider"})

    # Summary
    if not has_updates:
        blocks.append({
            "type": "section",
            "text": {
                "type": "mrkdwn",
                "text": "âœ… *Summary:* No significant changes detected"
            }
        })
    else:
        blocks.append({
            "type": "section",
            "text": {
                "type": "mrkdwn",
                "text": "ğŸ”” *Summary:* Updates detected! Check details above."
            }
        })

    return {"blocks": blocks}


def send_slack_notification(webhook_url, results):
    """Send formatted results to Slack"""

    try:
        message = format_slack_message(results)

        response = requests.post(
            webhook_url,
            json=message,
            headers={'Content-Type': 'application/json'}
        )

        if response.status_code == 200:
            print("\nâœ… Slack notification sent successfully!")
            return True
        else:
            print(f"\nâŒ Failed to send Slack notification: {response.status_code}")
            print(f"Response: {response.text}")
            return False

    except Exception as e:
        print(f"\nâŒ Error sending Slack notification: {e}")
        return False


def main():
    """Main execution function"""

    # Get Slack webhook URL from environment variable
    webhook_url = os.environ.get('SLACK_WEBHOOK_URL')

    # Check if running in test mode
    test_mode = os.environ.get('TEST_MODE', '').lower() in ['true', '1', 'yes']

    if not webhook_url and not test_mode:
        print("âŒ ERROR: SLACK_WEBHOOK_URL environment variable not set")
        print("Please set it in ~/.bashrc or the cron script")
        print("\nOr run in test mode:")
        print("  TEST_MODE=true python3 daily_crawler.py")
        sys.exit(1)

    print("=" * 60)
    print(f"ğŸš€ Starting Daily Company Crawler")
    print(f"â° Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S KST')}")
    print("=" * 60)

    # Run crawler
    results = crawl_all_companies(purpose="all")

    # Print results to console
    print("\n" + "=" * 60)
    print("ğŸ“Š CRAWL RESULTS")
    print("=" * 60)

    for result in results:
        if result:
            company = result.get("company", "Unknown")
            print(f"\n[{company}]")
            if "error" in result:
                print(f"  âŒ Error: {result['error']}")
            else:
                print(f"  âœ… Success")

    # Send Slack notification
    if webhook_url:
        print("\n" + "=" * 60)
        print("ğŸ“¤ Sending Slack Notification...")
        print("=" * 60)

        send_slack_notification(webhook_url, results)
    elif test_mode:
        print("\n" + "=" * 60)
        print("âš ï¸  TEST MODE: Skipping Slack notification")
        print("=" * 60)

    print("\nâœ… Daily crawler completed!")
    print("=" * 60)


if __name__ == "__main__":
    main()
